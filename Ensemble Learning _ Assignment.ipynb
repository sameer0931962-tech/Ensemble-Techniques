{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCPHbU7ToN7M85JiP+v8aK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind i"],"metadata":{"id":"Bvb_7B6XRoAn"}},{"cell_type":"markdown","source":["**Ensemble Learning** is a machine learning approach where **several different models are trained on the same problem and their predictions are combined** to improve accuracy, reduce errors, and make the final model more reliable than a single model.\n"],"metadata":{"id":"vTPAfUaBRt4n"}},{"cell_type":"markdown","source":["# Question 2: What is the difference between Bagging and Boosting?"],"metadata":{"id":"JBp85QxGRwC3"}},{"cell_type":"markdown","source":["### Difference between **Bagging** and **Boosting**\n","\n","| Basis            | **Bagging (Bootstrap Aggregating)**                | **Boosting**                                                          |\n","| ---------------- | -------------------------------------------------- | --------------------------------------------------------------------- |\n","| Main idea        | Trains multiple models **independently**           | Trains models **sequentially**                                        |\n","| Focus            | Reduces **variance**                               | Reduces **bias and variance**                                         |\n","| Data usage       | Uses **random samples** of data (with replacement) | Uses the **same data**, but gives more weight to misclassified points |\n","| Model dependency | Models are **not dependent** on each other         | Each model **depends on the previous one**                            |\n","| Handling errors  | All models are treated **equally**                 | Misclassified data gets **more importance**                           |\n","| Speed            | Can be trained **in parallel**                     | Training is **slower** (sequential)                                   |\n","| Example          | Random Forest                                      | AdaBoost, Gradient Boosting                                           |\n","\n","**In short:**\n","\n","* **Bagging** builds many independent models and averages their results.\n","* **Boosting** builds models step by step, focusing more on correcting previous mistakes.\n"],"metadata":{"id":"vMf_siSNR0Rf"}},{"cell_type":"markdown","source":["# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"],"metadata":{"id":"0hCLn7-5R42f"}},{"cell_type":"markdown","source":["**Bootstrap sampling** is a resampling technique where **multiple new datasets are created by randomly selecting data points from the original dataset with replacement**.\n","\n","### Role of Bootstrap Sampling in Bagging (e.g., Random Forest)\n","\n","In Bagging methods like **Random Forest**, bootstrap sampling:\n","\n","* Creates **different training datasets** for each model\n","* Helps models learn **slightly different patterns**\n","* Reduces **overfitting and variance**\n","* Improves **overall model stability and accuracy**\n","\n","**In simple words:**\n","Bootstrap sampling allows each tree in Random Forest to train on a **different version of the data**, making the combined model stronger and more reliable.\n"],"metadata":{"id":"tOXcXrhkR-Fy"}},{"cell_type":"markdown","source":["# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"],"metadata":{"id":"mmhpuxkySA4g"}},{"cell_type":"markdown","source":["**Out-of-Bag (OOB) samples** are the data points that **are not selected during bootstrap sampling** when training each model in a bagging-based ensemble.\n","\n","### How OOB Score Is Used\n","\n","In ensemble models like **Random Forest**:\n","\n","* Each model is trained on its own bootstrap sample\n","* The data not used for training that model (OOB samples) act as **test data**\n","* Predictions on OOB samples are aggregated\n","* The **OOB score** measures model performance (accuracy for classification, error for regression)\n","\n","### Why OOB Score Is Useful\n","\n","* No need for a **separate validation dataset**\n","* Provides a **reliable estimate** of model performance\n","* Saves **time and data**\n","\n","**In short:**\n","OOB samples work like a built-in test set to evaluate ensemble models efficiently.\n"],"metadata":{"id":"UTPrCppMSH3D"}},{"cell_type":"markdown","source":["# Question 5: Compare feature importance analysis in a single Decision Tree vs.  Random Forest."],"metadata":{"id":"NMhSdoE3SKFq"}},{"cell_type":"markdown","source":["### Feature Importance: **Decision Tree vs. Random Forest**\n","\n","| Aspect                       | **Single Decision Tree**                                 | **Random Forest**                                |\n","| ---------------------------- | -------------------------------------------------------- | ------------------------------------------------ |\n","| How importance is calculated | Based on **impurity reduction** at each split            | **Averaged** impurity reduction across all trees |\n","| Stability                    | **Unstable** (small data change → big importance change) | **More stable and reliable**                     |\n","| Overfitting effect           | Can give **misleading importance** due to overfitting    | Reduces overfitting by using many trees          |\n","| Feature bias                 | Can favor features with more split points                | Bias is **reduced** due to randomness            |\n","| Overall reliability          | **Less reliable**                                        | **More reliable and robust**                     |\n","\n","### Summary\n","\n","* A **single Decision Tree** shows feature importance based on one model, so results can be noisy.\n","* A **Random Forest** combines feature importance from many trees, giving a **more accurate and trustworthy** importance ranking.\n"],"metadata":{"id":"hbd63hllSN4b"}},{"cell_type":"markdown","source":["# Question 6: Write a Python program to:\n","● Load the Breast Cancer dataset using\n","sklearn.datasets.load_breast_cancer()\n","● Train a Random Forest Classifier\n","● Print the top 5 most important features based on feature importance scores. (Include your Python code and output in the code box below.)"],"metadata":{"id":"1eYaq3TjSatL"}},{"cell_type":"code","source":["# Import required libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load the Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","feature_names = data.feature_names\n","\n","# Train a Random Forest Classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X, y)\n","\n","# Get feature importance scores\n","importances = rf.feature_importances_\n","\n","# Create a DataFrame for better visualization\n","feature_importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Importance': importances\n","})\n","\n","# Sort features by importance (descending)\n","feature_importance_df = feature_importance_df.sort_values(\n","    by='Importance', ascending=False\n",")\n","\n","# Print top 5 most important features\n","print(\"Top 5 Most Important Features:\")\n","print(feature_importance_df.head(5))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6joplLZsSq44","executionInfo":{"status":"ok","timestamp":1768306706663,"user_tz":-330,"elapsed":5503,"user":{"displayName":"Sameer","userId":"04439344953953225908"}},"outputId":"65dece3c-a3f6-4324-b1a8-e802b43722c7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 Most Important Features:\n","                 Feature  Importance\n","23            worst area    0.139357\n","27  worst concave points    0.132225\n","7    mean concave points    0.107046\n","20          worst radius    0.082848\n","22       worst perimeter    0.080850\n"]}]},{"cell_type":"markdown","source":["# Question 7: Write a Python program to:\n","● Train a Bagging Classifier using Decision Trees on the Iris dataset\n","● Evaluate its accuracy and compare with a single Decision Tree\n","(Include your Python code and output in the code box below.)"],"metadata":{"id":"AeZilsmwSoNG"}},{"cell_type":"code","source":["# Import required libraries\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# Single Decision Tree\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train, y_train)\n","dt_pred = dt.predict(X_test)\n","dt_accuracy = accuracy_score(y_test, dt_pred)\n","\n","# Bagging Classifier (FIX: use 'estimator' instead of 'base_estimator')\n","bagging = BaggingClassifier(\n","    estimator=DecisionTreeClassifier(),\n","    n_estimators=50,\n","    random_state=42\n",")\n","bagging.fit(X_train, y_train)\n","bagging_pred = bagging.predict(X_test)\n","bagging_accuracy = accuracy_score(y_test, bagging_pred)\n","\n","# Print results\n","print(\"Decision Tree Accuracy:\", dt_accuracy)\n","print(\"Bagging Classifier Accuracy:\", bagging_accuracy)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAc-4JhvTEdC","executionInfo":{"status":"ok","timestamp":1768306852289,"user_tz":-330,"elapsed":360,"user":{"displayName":"Sameer","userId":"04439344953953225908"}},"outputId":"6ee45d69-08f9-40bf-d82d-f5aabdad0cf6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Accuracy: 1.0\n","Bagging Classifier Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["# Question 8: Write a Python program to:\n","● Train a Random Forest Classifier\n","● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n","● Print the best parameters and final accuracy\n","(Include your Python code and output in the code box below.)"],"metadata":{"id":"Xibg32k1TVRa"}},{"cell_type":"markdown","source":["Below is a clean, updated (scikit-learn ≥ 1.2), and exam-ready Python program that trains a Random Forest, tunes max_depth and n_estimators using GridSearchCV, and prints the best parameters and final accuracy."],"metadata":{"id":"3aHPbkE6TgCT"}},{"cell_type":"code","source":["# Import required libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# Define Random Forest model\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Define hyperparameter grid\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 5, 10, 20]\n","}\n","\n","# GridSearchCV\n","grid_search = GridSearchCV(\n","    estimator=rf,\n","    param_grid=param_grid,\n","    cv=5,\n","    scoring='accuracy',\n","    n_jobs=-1\n",")\n","\n","# Train GridSearch\n","grid_search.fit(X_train, y_train)\n","\n","# Best model\n","best_model = grid_search.best_estimator_\n","\n","# Predictions and accuracy\n","y_pred = best_model.predict(X_test)\n","final_accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print results\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Final Accuracy:\", final_accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3s_m-HhTkRD","executionInfo":{"status":"ok","timestamp":1768306948988,"user_tz":-330,"elapsed":21224,"user":{"displayName":"Sameer","userId":"04439344953953225908"}},"outputId":"40db8dd2-e1e4-45c3-8daf-db9d1aca7ea4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'max_depth': None, 'n_estimators': 200}\n","Final Accuracy: 0.9707602339181286\n"]}]},{"cell_type":"markdown","source":["# Question 9: Write a Python program to:\n","● Train a Bagging Regressor and a Random Forest Regressor on the California\n","Housing dataset\n","● Compare their Mean Squared Errors (MSE)\n","(Include your Python code and output in the code box below.)"],"metadata":{"id":"2gxMpB_cTstT"}},{"cell_type":"code","source":["# Import required libraries\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Load California Housing dataset\n","data = fetch_california_housing()\n","X = data.data\n","y = data.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# Train Bagging Regressor\n","bagging_reg = BaggingRegressor(\n","    estimator=DecisionTreeRegressor(),\n","    n_estimators=50,\n","    random_state=42\n",")\n","bagging_reg.fit(X_train, y_train)\n","bagging_pred = bagging_reg.predict(X_test)\n","bagging_mse = mean_squared_error(y_test, bagging_pred)\n","\n","# Train Random Forest Regressor\n","rf_reg = RandomForestRegressor(\n","    n_estimators=100,\n","    random_state=42\n",")\n","rf_reg.fit(X_train, y_train)\n","rf_pred = rf_reg.predict(X_test)\n","rf_mse = mean_squared_error(y_test, rf_pred)\n","\n","# Print results\n","print(\"Bagging Regressor MSE:\", bagging_mse)\n","print(\"Random Forest Regressor MSE:\", rf_mse)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVhm8-biTxLt","executionInfo":{"status":"ok","timestamp":1768307018998,"user_tz":-330,"elapsed":32348,"user":{"displayName":"Sameer","userId":"04439344953953225908"}},"outputId":"eebdff8f-6602-4b37-984e-e1f3ef3f1e24"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Regressor MSE: 0.25787382250585034\n","Random Forest Regressor MSE: 0.25650512920799395\n"]}]},{"cell_type":"markdown","source":["# Question 10: You are working as a data scientist at a financial institution to predict loan\n","default. You have access to customer demographic and transaction history data.\n","You decide to use ensemble techniques to increase model performance.\n","Explain your step-by-step approach to:\n","● Choose between Bagging or Boosting\n","● Handle overfitting\n","● Select base models\n","● Evaluate performance using cross-validation\n","● Justify how ensemble learning improves decision-making in this real-world\n","context."],"metadata":{"id":"PSmP-zazT6b3"}},{"cell_type":"code","source":["# Import required libraries\n","from sklearn.datasets import load_breast_cancer   # proxy for loan default data\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","\n","# Load dataset (simulating loan default data)\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","# Train Boosting model\n","gb_model = GradientBoostingClassifier(\n","    n_estimators=100,\n","    learning_rate=0.1,\n","    max_depth=3,\n","    random_state=42\n",")\n","gb_model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = gb_model.predict(X_test)\n","y_prob = gb_model.predict_proba(X_test)[:, 1]\n","\n","# Evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","roc_auc = roc_auc_score(y_test, y_prob)\n","\n","# Cross-validation\n","cv_scores = cross_val_score(gb_model, X, y, cv=5, scoring='accuracy')\n","\n","# Print results\n","print(\"Test Accuracy:\", accuracy)\n","print(\"ROC-AUC Score:\", roc_auc)\n","print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n","print(\"Mean CV Accuracy:\", cv_scores.mean())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53XoxMdyTZei","executionInfo":{"status":"ok","timestamp":1768307144823,"user_tz":-330,"elapsed":9652,"user":{"displayName":"Sameer","userId":"04439344953953225908"}},"outputId":"b3735265-dae3-49aa-991b-18498a2888ec"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.9590643274853801\n","ROC-AUC Score: 0.9951499118165785\n","Cross-Validation Accuracy Scores: [0.92982456 0.94736842 0.97368421 0.98245614 0.98230088]\n","Mean CV Accuracy: 0.9631268436578171\n"]}]}]}